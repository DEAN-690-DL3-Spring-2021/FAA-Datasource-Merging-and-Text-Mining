# -*- coding: utf-8 -*-
"""Merging.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i4PMC8MYZD_Xhmufk7HHW_g0i_UR3w2L
"""

import pandas as pd
import numpy as np

"""## AID Data Analysis"""

aid_data = pd.read_csv('a2019.csv',encoding='UTF-8',skipinitialspace=True)

aid_data1 = aid_data

aid_data1.columns= aid_data1.columns.str.strip()

# aid_data1 = aid_data1.dropna(subset=['c5'],how='all')

aid_data1 = aid_data1.drop(aid_data1.columns[[-1,-2]],axis=1)

fil = aid_data1['c5'].str.contains(r'^[a-zA-Z]')

aid_data1 = aid_data1[~fil]

aid_data1.reset_index(drop=True,inplace=True)

aid_data1['c9'] = pd.to_datetime(aid_data1['c9'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c205'] = pd.to_datetime(aid_data1['c205'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c206'] = pd.to_datetime(aid_data1['c206'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c207'] = pd.to_datetime(aid_data1['c207'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c208'] = pd.to_datetime(aid_data1['c208'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c210'] = pd.to_datetime(aid_data1['c210'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c229'] = pd.to_datetime(aid_data1['c229'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c230'] = pd.to_datetime(aid_data1['c230'].astype(str), format='%Y%m%d',errors='coerce')
aid_data1['c10'] = pd.to_datetime(aid_data1['c10'].astype(str),format= '%H%M.0',errors='coerce').dt.strftime('%H:%M')

aid_data1[aid_data1.columns[79:95]] = aid_data1[aid_data1.columns[79:95]].fillna(0)

for col in aid_data1.columns[80:93].values:
    aid_data1[col] = aid_data1[col].astype('int64')

aid_data1[['c149','c151','c145']] = aid_data1[['c149','c151','c145']].fillna(0).astype(int).astype(str).replace('0','NaN')

aid_data1.rename(columns={'c9': 'Date_Of_Occurrence', 'c22': 'N_Number'}, inplace=True)

aid_data1 = aid_data1.add_suffix('_aid').rename(columns={'Report_Type_aid': 'Report_Type', 'Date_Of_Occurrence_aid': 'Date_Of_Occurrence','N_Number_aid':'N_Number'})

aid_data1.insert(0,'Report_Type','AID')
    
# aid_data1[aid_data1.columns[79:95]]

aid_data1

# aid_data1.c151_aid

# aid_data1.to_csv('a2019_Clean.csv', index=False)

"""## SDR Data Analysis"""

sdr_data = pd.read_csv('sdr(2019_Occurred_Incidents).csv',encoding='UTF-8',skipinitialspace=True,dtype=str)
sdr_data1 = sdr_data

sdr_data1 = sdr_data1.dropna(subset=['c25'],how='all')

sdr_data1 = sdr_data1.drop(sdr_data1.columns[[-1,-2,-3,0]],axis=1)
sdr_data1.columns= sdr_data1.columns.str.strip()

sdr_data1['c10'] = pd.to_datetime(sdr_data1['c10'].astype(str), format='%Y%m%d',errors='coerce')
sdr_data1['c25'] = pd.to_datetime(sdr_data1['c25'].astype(str), format='%Y%m%d',errors='coerce')

sdr_data1['Remarks'] = sdr_data1[['c510a','c510b','c510c','c510d','c510e','c510f']].fillna('').agg(' '.join, axis=1)

sdr_data1['Remarks']= sdr_data1['Remarks'].str.strip().str.replace('\s+', ' ', regex=True)

sdr_data1.drop(['c510a', 'c510b','c510c','c510d','c510e','c510f'], axis=1, inplace=True)

sdr_data1.rename(columns={'c25': 'Date_Of_Occurrence', 'c390': 'N_Number'}, inplace=True)

sdr_data1 = sdr_data1.add_suffix('_sdr').rename(columns={'Report_Type_sdr': 'Report_Type', 'Date_Of_Occurrence_sdr': 'Date_Of_Occurrence','N_Number_sdr':'N_Number','Remarks_sdr':'Remarks'})

sdr_data1.insert(0,'Report_Type','SDR')

sdr_data1.head(5)

sdr_data1['Remarks'].loc[3000]

sdr_data1.shape

sdr_data1['Date_Of_Occurrence'].isnull().sum()

# sdr_data1.to_csv('sdr2019_Clean.csv', index=False)

"""## E-File AID Data Analysis"""

import more_itertools as mit

e_aid_data = pd.read_csv('e2019(possible).csv',encoding='UTF-8',skipinitialspace=True)

e_aid_data1= e_aid_data

e_aid_data1 = e_aid_data1.dropna(subset=['c5'],how='all')

e_aid_data1['remark'] = e_aid_data1[['remark','Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7','Unnamed: 8','Unnamed: 9']].replace('?','').fillna('').agg(' '.join, axis=1)
e_aid_data1 = e_aid_data1.drop(e_aid_data1.columns[[2,3,4,5,6,7,8,9]],axis=1)

e_aid_data1.reset_index(drop=True,inplace=True)

e_aid_data1['remark']= e_aid_data1['remark'].replace(r'^\s*$', np.nan, regex=True)

e_aid_data1['remark']= e_aid_data1['remark'].mask(pd.isnull, e_aid_data1['c5'])


fill = e_aid_data1['c5'].str.contains(r'^\b2019')

e_aid_data1['c5'][fill==False] = np.nan

q=[index for index, row in e_aid_data1.iterrows() if row.isnull().any()]

q=[list(group) for group in mit.consecutive_groups(q)]

for i in q:
    t=''
    s=i[0]
    for j in i:
        t+=(e_aid_data1.iloc[j]['remark'])
        t+=' '
    e_aid_data1.iloc[s-1]['remark']+=t
        
e_aid_data1 = e_aid_data1[pd.notnull(e_aid_data1['c5'])]

e_aid_data1.reset_index(drop=True,inplace=True)

e_aid_data1.rename(columns={'c5':'c5_aid','remark': 'Remarks'},inplace=True)

e_aid_data1['Remarks'] = e_aid_data1['Remarks'].str.replace('\^PRIVACY DATA OMITTED\^', '').str.strip().str.replace('\s+', ' ', regex=True)

# e_aid_data1['Remarks'] = e_aid_data1['Remarks'].str.replace('iaw srm dwg', '').str.strip().str.replace('\s+', ' ', regex=True)

# e_aid_data1 = e_aid_data1.add_suffix('_aid')

e_aid_data1.insert(0,'Report_Type','AID')

e_aid_data1

e_aid_data1['Remarks'].loc[1]

# e_aid_data1.to_csv('e2019(possible)_Clean.csv', index=False)

aid_data1 = pd.merge(aid_data1, e_aid_data1, how="outer", on=["Report_Type", "c5_aid"])

# aid_data1 = aid_data1[['Report_Type','Remarks']]

aid_data1 = aid_data1.drop('c119_aid',axis=1)

aid_data1

# aid_data1.to_csv('AID_a-e_file_combined.csv', index=False)

"""## EON Data Analysis"""

eon_data = pd.read_csv('EON-2019(Jan-Mar).csv',encoding='UTF-8',skipinitialspace=True)
eon_data1 = eon_data
eon_data1.columns= eon_data1.columns.str.strip()

eon_data1['Event Date'] = pd.to_datetime(eon_data1['Event Date'])
eon_data1['Event Time'] = eon_data1['Event Date'].dt.strftime('%H:%M')
eon_data1['Event Date'] = eon_data1['Event Date'].dt.strftime('%Y-%m-%d')

eon_data1['Event Date'] = pd.to_datetime(eon_data1['Event Date'])

cols = eon_data1.columns.tolist()
cols = cols[0:5]+cols[-1:]+cols[5:36]
eon_data1 = eon_data1[cols]

eon_data1['Message Whole'] = eon_data1[['MessageText.1','Unnamed: 30','Unnamed: 31','Unnamed: 32','Unnamed: 33','Unnamed: 34','Unnamed: 35']].fillna('').agg(' '.join, axis=1)

eon_data1['Message Whole'] = eon_data1['Message Whole'].str.strip().str.replace('\s+', ' ', regex=True)

eon_data1.rename(columns={'Message Whole': 'Remarks','Event Date': 'Date_Of_Occurrence'},inplace=True)

cols1 = eon_data1.columns.tolist()
cols1 = cols1[0:29] + cols1[-1:]

eon_data1 = eon_data1[cols1]

eon_data1['N_Number'] = eon_data1['Title'].str.extract(r'((?<=\|).+?(?=\|))')

eon_data1['N_Number'] = eon_data1['N_Number'].str.strip()

fil = eon_data1['N_Number'].str.contains('^N[0-9]')

eon_data1['N_Number'][fil==True] = eon_data1['N_Number'][fil==True].apply(lambda x : x[1:] if str(x).startswith("N") else x)

eon_data1.insert(0,'Report_Type','EON')

# eon_data1.drop(['Title'], axis=1, inplace=True)

eon_data1.head(5)

# eon_data1['N-Number'].isna().sum()

# eon_obj = eon_data1.select_dtypes('object')

# eon_obj

eon_data1['Title'].loc[1]

# eon_data1.info()

# eon_data1.to_csv('EON-2019(Jan-Mar)_Clean.csv', index=False)

"""## Merging Process(in Progress)"""

merged_aid_sdr = aid_data1.append(sdr_data1,sort=False)

merged_aid_sdr

# merged_eon_eaid = eon_data1.append(e_aid_data1,sort=False)

# merged_eon_eaid

merged_total = merged_aid_sdr.append(eon_data1,sort=False)

merged_total

merged_total.reset_index(drop=True,inplace=True)

"""## Natural Language Processing"""

aid_data1_a = aid_data1[['Report_Type','c5_aid']]

aid_data1_a.shape

e_aid_data1.shape

aid_data1_nlp = pd.merge(aid_data1_a, e_aid_data1, how="outer", on=["Report_Type", "c5_aid"])

aid_data1_nlp = aid_data1_nlp[['Report_Type','Remarks']]

aid_data1_nlp

aid_text = aid_data1_nlp['Remarks'].tolist()
aid_text

# pip install scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer

tfIdfVectorizer=TfidfVectorizer(use_idf=True)
tfIdf = tfIdfVectorizer.fit_transform(aid_text)
aid_TF_IDF = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=["TF-IDF"])
aid_TF_IDF = aid_TF_IDF.sort_values('TF-IDF', ascending=False)
print (aid_TF_IDF.head(25))

# aid_data1_nlp.Remarks[aid_data1_nlp.Remarks.str.contains('(SMOKE)|(FUMES)|(FIRE)|(ODORS)|(SPARKE)')]

# aid_data1_nlp.Remarks[aid_data1_nlp.Remarks.str.contains('(BIRD STRIKE)')]

# aid_data1_nlp.Remarks[aid_data1_nlp.Remarks.str.contains('(FUSELAGE)')]

aid_data1_nlp.Remarks[aid_data1_nlp.Remarks.str.contains('(ENGINE FIRE)')]

# aid_data1_nlp.info()

# aid_data1_nlp.to_csv('a_2019(NLP).csv', index=False)

sdr_data1_nlp = sdr_data1[['Report_Type','Remarks']]

eon_data1_nlp = eon_data1[['Report_Type','Remarks']]

eon_data1_nlp.Remarks = eon_data1_nlp.Remarks.str.lstrip(',').str.strip()

# sdr_data1_nlp.Remarks[sdr_data1_nlp.Remarks.str.contains('(BIRD STRIKE)')]

# sdr_data1_nlp.Remarks[sdr_data1_nlp.Remarks.str.contains('(FUSELAGE)')]

sdr_data1_nlp.Remarks[sdr_data1_nlp.Remarks.str.contains('((ENGINE FIRE))')]

(sdr_data1['c250_sdr']+' '+sdr_data1['c260_sdr']).value_counts().head(20)

# sdr_data1_nlp.Remarks[sdr_data1_nlp.Remarks.str.contains('(SMOKE)|(FUMES)|(FIRE)|(ODORS)|(SPARKE)')]

# eon_data1_nlp.Remarks[eon_data1_nlp.Remarks.str.contains('(BIRD STRIKE)')]

# eon_data1_nlp.Remarks[eon_data1_nlp.Remarks.str.contains('(FUSELAGE)')]

eon_data1_nlp.Remarks[eon_data1_nlp.Remarks.str.contains('(ENGINE FIRE)')]

# eon_data1_nlp.Remarks[eon_data1_nlp.Remarks.str.contains('(SMOKE)|(FUMES)|(FIRE)|(ODORS)|(SPARKE)')]

aid_sdr_eon_nlp = aid_data1_nlp.append([sdr_data1_nlp, eon_data1_nlp])

aid_sdr_eon_nlp.reset_index(drop=True,inplace=True)

aid_sdr_eon_nlp.columns = aid_sdr_eon_nlp.columns.str.strip().str.replace('\s+', ' ', regex=True)

aid_sdr_eon_nlp









"""## Text Analysis"""

import re
import unicodedata
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')
import matplotlib.pyplot as plt
# add appropriate words that will be ignored in the analysis
# ADDITIONAL_STOPWORDS = ['covfefe']

def basic_clean(text):
  """
  A simple function to clean up the data. All the words that
  are not designated as a stop word is then lemmatized after
  encoding and basic regex parsing are performed.
  """
  wnl = nltk.stem.WordNetLemmatizer()
  stopwords = nltk.corpus.stopwords.words('english')
  text = (unicodedata.normalize('NFKD', text)
    .encode('ascii', 'ignore')
    .decode('utf-8', 'ignore')
    .lower())
  words = re.sub(r'[^\w\s]', '', text).split()
  return [wnl.lemmatize(word) for word in words if word not in stopwords]

words_aid = basic_clean(''.join(str(aid_data1_nlp['Remarks'].tolist())))

words_sdr = basic_clean(''.join(str(sdr_data1['Remarks'].tolist())))

words_eon = basic_clean(''.join(str(eon_data1['Remarks'].tolist())))

(pd.Series(nltk.ngrams(words_aid, 2)).value_counts())[:60]

(pd.Series(nltk.ngrams(words_sdr, 4)).value_counts())[:60]

(pd.Series(nltk.ngrams(words_eon, 4)).value_counts())[:60]

"""## Merging Trials"""

aid_trial = aid_data1[['Report_Type','Date_Of_Occurrence','c23_aid','c24_aid','c95_aid','N_Number','Remarks']]

sdr_trial = sdr_data1[['Report_Type','Date_Of_Occurrence','c130_sdr','c140_sdr','c100_sdr','c250_sdr','c260_sdr','c332_sdr','N_Number','Remarks']]

eon_trial = eon_data1[['Report_Type','Date_Of_Occurrence','Event Time','N_Number','Aircraft Type','Remarks']]

aid_trial.dtypes, sdr_trial.dtypes,eon_trial.dtypes

"""## AID-SDR Merge """

aid_sdr_merge = pd.merge(aid_trial, sdr_trial,on=["N_Number"])
aid_sdr_merge

aid_sdr_merge[aid_sdr_merge.duplicated()]

aid_sdr_merge.c24_aid = aid_sdr_merge.c24_aid.str.replace('-','').str.strip().str.replace(' ', '', regex=True).astype(str)
aid_sdr_merge.c140_sdr = aid_sdr_merge.c140_sdr.str.strip().str.replace(' ', '', regex=True).astype(str)
aid_sdr_merge.c130_sdr = aid_sdr_merge.c130_sdr.str.strip().str.replace(' ', '', regex=True).astype(str)
aid_sdr_merge

for i in range(len(aid_sdr_merge)):
    if aid_sdr_merge.loc[i,'c24_aid'].startswith(aid_sdr_merge.loc[i,'c140_sdr']) | aid_sdr_merge.loc[i,'c24_aid'].startswith(aid_sdr_merge.loc[i,'c130_sdr']):
        aid_sdr_merge.loc[i,'c24_aid'] = aid_sdr_merge.loc[i,'c140_sdr']
        
aid_sdr_merge

aid_sdr_merge.loc[0,'c24_aid']

len(aid_sdr_merge)

aid_sdr_merge_sameDOC = aid_sdr_merge[aid_sdr_merge['Date_Of_Occurrence_x']==aid_sdr_merge['Date_Of_Occurrence_y']]
aid_sdr_merge_sameDOC

len(aid_sdr_merge_sameDOC)

aid_sdr_merge1 = pd.merge(sdr_trial,aid_trial, how='left', on=["Date_Of_Occurrence","N_Number"])
aid_sdr_merge1

len(aid_sdr_merge1)

len(aid_trial) + len(sdr_trial)

"""## AID_SDR_EON_Merge """

aid_sdr_eon_merge = pd.merge(aid_sdr_merge, eon_trial,on="N_Number")
aid_sdr_eon_merge

aid_sdr_eon_merge.loc[2,'Remarks']

aid_sdr_eon_sameDOC_merge = pd.merge(aid_sdr_merge_sameDOC, eon_trial, on="N_Number")
aid_sdr_eon_sameDOC_merge

aid_sdr_eon_sameDOC_merge.query('Date_Of_Occurrence_x==Date_Of_Occurrence')

# aid_sdr_eon_sameDOC_merge.dtypes

"""## SDR_EON_Merge """

sdr_eon_merge = pd.merge(sdr_trial, eon_trial, on="N_Number")
# sdr_eon_merge

sdr_eon_merge1 = sdr_eon_merge[~sdr_eon_merge['N_Number'].isna()]
sdr_eon_merge1

# sdr_eon_merge.loc[364590,'Remarks']

sdr_eon_merge1.query('Date_Of_Occurrence_x==Date_Of_Occurrence_y')



"""## Fuzzy Matching """

pip install fuzzymatcher

aid_fuzzy = aid_data1[['Report_Type','c5_aid','Date_Of_Occurrence','c13_aid','c23_aid','c24_aid','c95_aid','N_Number','Remarks']]

sdr_fuzzy = sdr_data1[['Report_Type','c14_sdr','c18_sdr','Date_Of_Occurrence','c130_sdr','c140_sdr','c100_sdr','c250_sdr','c260_sdr','c332_sdr','N_Number','Remarks']]

eon_fuzzy = eon_data1[['Report_Type','Id','Date_Of_Occurrence','N_Number','Aircraft Type','Remarks']]

print(aid_fuzzy.head(5))
print(sdr_fuzzy.head(5))
print(eon_fuzzy.head(5))

aid_fuzzy.columns.to_list()

sdr_fuzzy.columns.to_list()

eon_fuzzy.columns.to_list()

aid_fuzzy['Aircraft_Type'] = aid_fuzzy['c23_aid'] + ' '+ aid_fuzzy['c24_aid']
aid_fuzzy['Aircraft_Type'] = aid_fuzzy['Aircraft_Type'].str.strip().str.replace('\s+', ' ', regex=True).astype(str)
aid_fuzzy['Aircraft_Type']

aid_fuzzy = aid_fuzzy.drop(['c23_aid','c24_aid'],axis=1)

aid_fuzzy.rename(columns={'Date_Of_Occurrence': 'Date_Of_Occurrence_aid', 'N_Number': 'N_Number_aid','Aircraft_Type':'Aircraft_Type_aid','Remarks':'Remarks_aid'}, inplace=True)

sdr_fuzzy['Aircraft_Type'] = sdr_fuzzy['c130_sdr'] + ' '+ sdr_fuzzy['c140_sdr']
sdr_fuzzy['Aircraft_Type'] = sdr_fuzzy['Aircraft_Type'].str.strip().str.replace('\s+', ' ', regex=True).astype(str)
sdr_fuzzy['Aircraft_Type']

sdr_fuzzy['Defective_Part_Description'] = sdr_fuzzy['c100_sdr'] + ' ' + sdr_fuzzy['c250_sdr'] + ' ' + sdr_fuzzy['c260_sdr']
sdr_fuzzy['Defective_Part_Description'] = sdr_fuzzy['Defective_Part_Description'].str.strip().str.replace('\s+', ' ', regex=True).astype(str)
sdr_fuzzy['Defective_Part_Description']

sdr_fuzzy = sdr_fuzzy.drop(['c100_sdr','c130_sdr','c140_sdr','c250_sdr','c260_sdr'],axis=1)

sdr_fuzzy['c18_sdr'] = sdr_fuzzy['c18_sdr'].str.replace('\W', '')

sdr_fuzzy.rename(columns={'Date_Of_Occurrence': 'Date_Of_Occurrence_sdr', 'N_Number': 'N_Number_sdr','Aircraft_Type':'Aircraft_Type_sdr','Remarks':'Remarks_sdr'}, inplace=True)

import fuzzymatcher

left_on = ["Date_Of_Occurrence_aid", "Aircraft_Type_aid", "N_Number_aid", "c95_aid"]

right_on = ["Date_Of_Occurrence_sdr", "Aircraft_Type_sdr", "N_Number_sdr", "c332_sdr"]

matched_results = fuzzymatcher.fuzzy_left_join(aid_fuzzy,
                                            sdr_fuzzy,
                                            left_on,
                                            right_on,
                                            left_id_col='c5_aid',
                                            right_id_col='c18_sdr')

cols = ["best_match_score", 'c5_aid', 'c18_sdr', "Date_Of_Occurrence_aid", "Date_Of_Occurrence_sdr", "Aircraft_Type_aid", "Aircraft_Type_sdr",
        "N_Number_aid", "N_Number_sdr", "c95_aid", "c332_sdr","Remarks_aid","Remarks_sdr"]

matched_results[cols].sort_values(by=['best_match_score'], ascending=False).head(50)

#matched_results.Remarks_aid.loc[165328]

#matched_results.Remarks_sdr.loc[175623]

fil = eon_fuzzy['N_Number'].str.contains(r'^[A-Z]',na=False)
eon_fuzzy = eon_fuzzy[~fil]

eon_fuzzy.reset_index(drop=True,inplace=True)

eon_fuzzy.rename(columns={'Date_Of_Occurrence': 'Date_Of_Occurrence_eon', 'N_Number': 'N_Number_eon','Aircraft Type':'Aircraft_Type_eon','Remarks':'Remarks_eon'}, inplace=True)

eon_fuzzy

left_on = ["Date_Of_Occurrence_aid", "Aircraft_Type_aid", "N_Number_aid"]

right_on = ["Date_Of_Occurrence_eon", "Aircraft_Type_eon", "N_Number_eon"]

matched_results = fuzzymatcher.fuzzy_left_join(aid_fuzzy,
                                            eon_fuzzy,
                                            left_on,
                                            right_on,
                                            left_id_col='c5_aid',
                                            right_id_col='Id')

cols = ["best_match_score", 'c5_aid', 'Id', "Date_Of_Occurrence_aid", "Date_Of_Occurrence_eon", "Aircraft_Type_aid", "Aircraft_Type_eon",
        "N_Number_aid", "N_Number_eon","Remarks_aid","Remarks_eon"]

matched_results[cols].sort_values(by=['best_match_score'], ascending=False).head(10)

left_on = ["Date_Of_Occurrence_eon", "Aircraft_Type_eon", "N_Number_eon"]

right_on = ["Date_Of_Occurrence_sdr", "Aircraft_Type_sdr", "N_Number_sdr"]

matched_results = fuzzymatcher.fuzzy_left_join(eon_fuzzy,
                                            sdr_fuzzy,
                                            left_on,
                                            right_on,
                                            left_id_col='Id',
                                            right_id_col='c18_sdr')

cols = ["best_match_score", 'Id', 'c18_sdr', "Date_Of_Occurrence_eon", "Date_Of_Occurrence_sdr", "Aircraft_Type_eon", "Aircraft_Type_sdr",
        "N_Number_eon", "N_Number_sdr","Remarks_eon","Remarks_sdr"]

matched_results[cols].sort_values(by=['best_match_score'], ascending=False).head(10)

"""# Topic Modelling"""

aid_topic = aid_data1[['Report_Type','c5_aid','Date_Of_Occurrence','c23_aid','c24_aid','N_Number','Remarks']]

sdr_topic = sdr_data1[['Report_Type','c18_sdr','Date_Of_Occurrence','c130_sdr','c140_sdr','N_Number','Remarks']]

eon_topic = eon_data1[['Report_Type','Id','Date_Of_Occurrence','N_Number','Aircraft Type','Remarks']]

aid_topic['Aircraft_Type'] = aid_topic['c23_aid'] + ' '+ aid_topic['c24_aid']
aid_topic['Aircraft_Type'] = aid_topic['Aircraft_Type'].str.strip().str.replace('\s+', ' ', regex=True).astype(str)
aid_topic['Aircraft_Type']

aid_topic = aid_topic.drop(['c23_aid','c24_aid'],axis=1)

aid_topic.rename(columns={'c5_aid': 'Unique Identifier'}, inplace=True)

sdr_topic['Aircraft_Type'] = sdr_topic['c130_sdr'] + ' '+ sdr_topic['c140_sdr']
sdr_topic['Aircraft_Type'] = sdr_topic['Aircraft_Type'].str.strip().str.replace('\s+', ' ', regex=True).astype(str)
sdr_topic['Aircraft_Type']

# sdr_topic['Defective_Part_Description'] = sdr_topic['c100_sdr'] + ' ' + sdr_topic['c250_sdr'] + ' ' + sdr_topic['c260_sdr']
# sdr_topic['Defective_Part_Description'] = sdr_topic['Defective_Part_Description'].str.strip().str.replace('\s+', ' ', regex=True).astype(str)
# sdr_topic['Defective_Part_Description']

sdr_topic = sdr_topic.drop(['c130_sdr','c140_sdr'],axis=1)

sdr_topic['c18_sdr'] = sdr_topic['c18_sdr'].str.replace('\W', '')

sdr_topic.rename(columns={'c18_sdr': 'Unique Identifier'}, inplace=True)

eon_topic.rename(columns={'Aircraft Type': 'Aircraft_Type','Id':'Unique Identifier'}, inplace=True)

aid_sdr_eon_topic = aid_topic.append([sdr_topic, eon_topic])

aid_sdr_eon_topic.reset_index(drop=True,inplace=True)

aid_sdr_eon_topic.columns = aid_sdr_eon_topic.columns.str.strip().str.replace('\s+', ' ', regex=True)

aid_sdr_eon_topic = aid_sdr_eon_topic[['Report_Type','Unique Identifier','Date_Of_Occurrence','Aircraft_Type','N_Number','Remarks']]

aid_sdr_eon_topic

aid_sdr_eon_topic.to_csv('aid_sdr_eon_topic_final.csv', index=False)

# Load the regular expression library
import re


# Remove punctuation
aid_sdr_eon_topic['Remarks'] = \
aid_sdr_eon_topic['Remarks'].map(lambda x: re.sub('[,\.!?]', '', x))
# Convert the titles to lowercase
aid_sdr_eon_topic['Remarks'] = \
aid_sdr_eon_topic['Remarks'].map(lambda x: x.lower())

aid_sdr_eon_topic['Remarks'] = \
aid_sdr_eon_topic['Remarks'].str.replace('iaw','in accordance with').astype(str)
aid_sdr_eon_topic['Remarks'] = \
aid_sdr_eon_topic['Remarks'].str.replace('srm','safety risk management').astype(str)
aid_sdr_eon_topic['Remarks'] = \
aid_sdr_eon_topic['Remarks'].str.replace('amm','Aircraft Maintenence manual').astype(str)



# Print out the first rows of papers
aid_sdr_eon_topic['Remarks'].head()

# Import the wordcloud library
from wordcloud import WordCloud
# Join the different processed titles together.
long_string = ','.join(list(aid_sdr_eon_topic['Remarks'].values))
# Create a WordCloud object
wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')
# Generate a word cloud
wordcloud.generate(long_string)
# Visualize the word cloud
wordcloud.to_image()

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]
data = aid_sdr_eon_topic.Remarks.values.tolist()
data_words = list(sent_to_words(data))
# remove stop words
data_words = remove_stopwords(data_words)
print(data_words[:1][0][:30])

import gensim.corpora as corpora
# Create Dictionary
id2word = corpora.Dictionary(data_words)
# Create Corpus
texts = data_words
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])

from pprint import pprint
import random

random.seed(10)
# number of topics
num_topics = 40
# Build LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

pip install pyLDAvis

"""Sample code"""

import warnings
warnings.filterwarnings('ignore')

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

import spacy
spacy.load('en')
from spacy.lang.en import English
parser = English()

def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens

import nltk
nltk.download('wordnet')

from nltk.corpus import wordnet as wn
def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is None:
        return word
    else:
        return lemma
    
from nltk.stem.wordnet import WordNetLemmatizer
def get_lemma2(word):
    return WordNetLemmatizer().lemmatize(word)

nltk.download('stopwords')
en_stop = set(nltk.corpus.stopwords.words('english'))

def prepare_text_for_lda(text):
    tokens = tokenize(text)
    tokens = [token for token in tokens if len(token) > 4]
    tokens = [token for token in tokens if token not in en_stop]
    tokens = [get_lemma(token) for token in tokens]
    return tokens

import random
text_data = []
with open('aid_sdr_eon_topic_final.csv') as f:
    for line in f:
        tokens = prepare_text_for_lda(line)
        if random.random() > .99:
            print(tokens)
            text_data.append(tokens)

from gensim import corpora
dictionary = corpora.Dictionary(text_data)

corpus = [dictionary.doc2bow(text) for text in text_data]

import pickle
pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')



import pyLDAvis.gensim

"""**Visualizing 15 topics:**"""

import gensim
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

NUM_TOPICS = 15
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
ldamodel.save('model15.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

lda15 = gensim.models.ldamodel.LdaModel.load('model15.gensim')
lda_display15 = pyLDAvis.gensim.prepare(lda15, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display15)

"""**Visualizing 3 topics:**"""

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, passes=15)
ldamodel.save('model3.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')
lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display3)

"""**Visualizing** **10** **topics**:"""

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)
ldamodel.save('model10.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

lda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')
lda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display10)

"""**Visualizing 40 topics:**"""

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 40, id2word=dictionary, passes=15)
ldamodel.save('model40.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

lda40 = gensim.models.ldamodel.LdaModel.load('model40.gensim')
lda_display40 = pyLDAvis.gensim.prepare(lda40, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display40)

"""**LSA**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline

#documents = ["doc1.txt", "doc2.txt", "doc3.txt"] 
  
# raw documents to tf-idf matrix: 

vectorizer = TfidfVectorizer(stop_words='english', 
                             use_idf=True, 
                             smooth_idf=True)

# SVD to reduce dimensionality: 

svd_model = TruncatedSVD(n_components=100,  
                         algorithm='randomized',
                         n_iter=10)

# pipeline of tf-idf + SVD, fit to and applied to documents:

svd_transformer = Pipeline([('tfidf', vectorizer), 
                            ('svd', svd_model)])

svd_matrix = svd_transformer.fit_transform(aid_sdr_eon_topic['Remarks'])

# svd_matrix can later be used to compare documents, compare words, or compare queries with documents

svd_matrix

from gensim.corpora import Dictionary
dictionary = corpora.Dictionary(text_data)
from gensim.corpora.Dictionary import load_from_text, doc2bow
from gensim.corpora import MmCorpus
from gensim.models.ldamodel import LdaModel

#document = "This is some document..."

# load id->word mapping (the dictionary)
id2word = load_from_text('wiki_en_wordids.txt')

# load corpus iterator
mm = MmCorpus('wiki_en_tfidf.mm')

# extract 100 LDA topics, updating once every 10,000
lda = LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)

# use LDA model: transform new doc to bag-of-words, then apply lda
doc_bow = doc2bow(aid_sdr_eon_topic['Remarks'].split())
doc_lda = lda[doc_bow]

# doc_lda is vector of length num_topics representing weighted presence of each topic in the doc

#Topic modeling with Ngrams

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as  pd
from pprint import pprint# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel# spaCy for preprocessing
import spacy# Plotting tools
import pyLDAvis
import pyLDAvis.gensim
import matplotlib.pyplot as plt
# %matplotlib inline

def sent_to_words(sentences):
  for sentence in sentences:
    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations
data_words = list(sent_to_words(data))
print(data_words[:1])

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)
# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)
# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

# Define function for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

print(data_lemmatized[:1])

# Create Dictionary 
id2word = corpora.Dictionary(data_lemmatized)  
# Create Corpus 
texts = data_lemmatized  
# Term Document Frequency 
corpus = [id2word.doc2bow(text) for text in texts]  
# View 
print(corpus[:1])

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

# Print the keyword of topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  
# a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Visualize the topics
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
vis